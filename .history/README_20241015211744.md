<p align="center">

  <h1 align="center">FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes Reconstruction</h1>
  <p align="center">
    <a href="https://wangys16.github.io/">Yunsong Wang</a>,
    <a href="https://tianxinhuang.github.io/">Tianxin Huang</a>,
    <a href="https://hlinchen.github.io/">Hanlin Chen</a>,
    <a href="https://www.comp.nus.edu.sg/~leegh/">Gim Hee Lee</a>

  </p>

  <h2 align="center">NeurIPS 2024</h2>

  <h3 align="center"><a href="https://arxiv.org/pdf/2406.05774">arXiv</a>
  <div align="center"></div>
</p>

<p align="center">
  <a href="">
    <img src="./teaser/FreeSplat.jpg" alt="Logo" width="95%">
  </a>
</p>

# pixelSplat

This is the code for **FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes** by David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann.

https://github.com/dcharatan/pixelsplat/assets/13124225/de90101e-1bb5-42e4-8c5b-35922cae8f64

## Installation

To get started, follow [pixelSplat](https://github.com/dcharatan/pixelsplat) to create a virtual environment using Python 3.10+:

```bash
python3.10 -m venv venv
source venv/bin/activate
# Install these first! Also, make sure you have python3.11-dev installed if using Ubuntu.
pip install wheel torch torchvision torchaudio
pip install -r requirements.txt
```

If your system does not use CUDA 12.1 by default, see the troubleshooting tips below.

<details>
<summary>Troubleshooting</summary>
<br>

The Gaussian splatting CUDA code (`diff-gaussian-rasterization`) must be compiled using the same version of CUDA that PyTorch was compiled with. As of December 2023, the version of PyTorch you get when doing `pip install torch` was built using CUDA 12.1. If your system does not use CUDA 12.1 by default, you can try the following:

- Install a version of PyTorch that was built using your CUDA version. For example, to get PyTorch with CUDA 11.8, use the following command (more details [here](https://pytorch.org/get-started/locally/)):

```bash
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

- Install CUDA Toolkit 12.1 on your system. One approach (*try this at your own risk!*) is to install a second CUDA Toolkit version using the `runfile (local)` option [here](https://developer.nvidia.com/cuda-12-1-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=runfile_local). When you run the installer, disable the options that install GPU drivers and update the default CUDA symlinks. If you do this, you can point your system to CUDA 12.1 during installation as follows:

```bash
LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64 pip install -r requirements.txt
# If everything else was installed but you're missing diff-gaussian-rasterization, do:
LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64 pip install git+https://github.com/dcharatan/diff-gaussian-rasterization-modified
```
</details>

## Acquiring Datasets

FreeSplat is trained using 100 scenes from [ScanNet](http://www.scan-net.org) following [NeRFusion](https://github.com/jetd1/NeRFusion) and [SurfelNeRF](https://github.com/TencentARC/SurfelNeRF), and evaluated on ScanNet and [Replica](https://github.com/facebookresearch/Replica-Dataset) datasets.

You can download our preprocessed datasets [here](). The downloaded datasets should look like:
```
scannet
├─ train
│  ├─ scene0005_00
|  ├  ├─ color (RGB images)
│  ├  ├─ depth (depth images)
│  ├  ├─ intrinsic (intrinsics)
│  ├  └─ extrinsics.npy (camera extrinsics)
│  ├─ scene0020_00
│     ├─ ...
├─ test
│  ├─ ...
...
```

Our sampled views for evaluation on different settings are in ```assets/evaluation_index_{dataset}_{N}views.json```.

## Acquiring Pre-trained Checkpoints

You can find our pre-trained checkpoints [here]().

## Running the Code

### Training

The main entry point is `src/main.py`. To train FreeSplat on 2-views, 3-views, and FVT settings, you can respectively call:

```bash
python3 -m src.main +experiment=scannet/2views +output_dir=2views
```
```bash
python3 -m src.main +experiment=scannet/3views +output_dir=3views
```
```bash
python3 -m src.main +experiment=scannet/fvt +output_dir=fvt
```
The output will be saved in path ```outputs/***```.


### Evaluation

To evaluate pre-trained model on the ```[N]```-views setting on ```[DATASET]```, you can call:

```bash
python3 -m src.main +experiment=[DATASET]/[SETTING] mode=test dataset/view_sampler=evaluation checkpointing.load=[PATH_TO_CHECKPOINT] dataset.view_sampler.num_context_views=[N]
```

For example, to evaluate 3-views trained FreeSplat on ScanNet 10-views setting, you can run:

```bash
python3 -m src.main +experiment=scannet/3views mode=test dataset/view_sampler=evaluation checkpointing.load=[PATH_TO_CHECKPOINT] dataset.view_sampler.num_context_views=10
```

We also provide a whole scene reconstruction example that you can possibly run by:
```bash
python3 -m src.main +experiment=scannet/fvt mode=test dataset/view_sampler=evaluation checkpointing.load=[PATH_TO_CHECKPOINT] dataset.view_sampler.num_context_views=30 model.encoder.num_views=30
```
where ```model.encoder.num_views=30``` is to use more nearby views during cost volume formulation for more accurate depth estimation.

## Camera Ready Updates

Before camera ready, we have re-ran the experiments and further improved our model. Our current version directly uses the features extracted by the backbone to conduct multi-view matching, achieving faster training and better performance with slight GPU overhead. Besides, we updated MVSplat's results after more careful hyperparameter tuning. Currently, FreeSplat still outperforms MVSplat on all settings and can easily perform longer sequence reconstruction.


## BibTeX
If you find our work helpful, please consider citing our paper. Thank you!
```
@article{wang2024freesplat,
  title={FreeSplat: Generalizable 3D Gaussian Splatting Towards Free-View Synthesis of Indoor Scenes},
  author={Wang, Yunsong and Huang, Tianxin and Chen, Hanlin and Lee, Gim Hee},
  journal={arXiv preprint arXiv:2405.17958},
  year={2024}
}
```

## Acknowledgements

This work is supported by the Agency for Science, Technology and Research (A*STAR) under its MTC Programmatic Funds (Grant No. M23L7b0021).
