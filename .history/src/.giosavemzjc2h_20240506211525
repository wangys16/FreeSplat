import os
from pathlib import Path

import hydra
import torch
import wandb
from colorama import Fore
from jaxtyping import install_import_hook
from omegaconf import DictConfig, OmegaConf
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint
from pytorch_lightning.loggers.wandb import WandbLogger

import argparse



# Configure beartype and jaxtyping.
# with install_import_hook(
#     ("src",),
#     ("beartype", "beartype"),
# ):
from src.config import load_typed_root_config
from src.dataset.data_module import DataModule
from src.global_cfg import set_cfg
from src.loss import get_losses
from src.misc.LocalLogger import LocalLogger
from src.misc.step_tracker import StepTracker
from src.misc.wandb_tools import update_checkpoint_path
from src.model.decoder import get_decoder
from src.model.encoder import get_encoder
from src.model.model_wrapper import ModelWrapper


def cyan(text: str) -> str:
    return f"{Fore.CYAN}{text}{Fore.RESET}"


@hydra.main(
    version_base=None,
    config_path="../config",
    config_name="main",
)



def train(cfg_dict: DictConfig):
    # print('cfg_dict:', cfg_dict)
    # parser = argparse.ArgumentParser()
    # parser.add_argument('output_dir', nargs="*", default=None)
    # FLAGS = parser.parse_args()
    # for k1 in cfg_dict:
    #     try:
    #         keys = cfg_dict[k1].keys()
    #         print(f'{k1}:')
    #         for k2 in keys:
    #             print(f'    {k2}: {cfg_dict[k1][k2]}')
    #     except:
    #         print(f'{k1}: {cfg_dict[k1]}')
    # new_cfg = cfg_dict.copy()
    cfg = load_typed_root_config(cfg_dict)
    # new_key = {}
    # for key in new_cfg.dataset.view_sampler.keys():
    #     new_key[key] = new_cfg.dataset.view_sampler[key]
    # new_key['name'] = 'evaluation'
    # new_cfg.dataset.view_sampler = DictConfig(new_key)
    # cfg_dict = DictConfig()
    # print('new_cfg:', new_cfg)
    # cfg_eval = load_typed_root_config(new_cfg)
    set_cfg(cfg_dict)
    torch.manual_seed(cfg_dict.seed)


    

    # # Set up the output directory.
    # if FLAGS.output_dir is None:

    # 
    if cfg.output_dir is None:
        output_dir = Path(
            hydra.core.hydra_config.HydraConfig.get()["runtime"]["output_dir"]
        )
        print(cyan(f"Saving outputs to {output_dir}."))
        latest_run = output_dir.parents[1] / "latest-run"
    else:
        output_dir = Path(f'../outputs/{cfg.output_dir}')
        if not os.path.exists(f'../outputs/{cfg.output_dir}'):
            os.mkdir(f'../outputs/{cfg.output_dir}')
        print(cyan(f"Saving outputs to {output_dir}."))
        latest_run = Path("../outputs") / "latest-run"


    run = wandb.init(dir=output_dir)
    # run.log_code(".")

    print('+++++++++++++++++run.dir:', run.dir)

    # LOG_FOUT = open(os.path.join(LOG_DIR, 'log_train.txt'), 'a')
    # else:
    #     output_dir = Path(f'outputs/{FLAGS.output_dir}')
    
    os.system(f"rm {latest_run}")
    os.system(f"ln -s {output_dir} {latest_run}")

    # Set up logging with wandb.
    callbacks = []
    if cfg_dict.wandb.mode != "disabled":
        logger = WandbLogger(
            project=cfg_dict.wandb.project,
            mode=cfg_dict.wandb.mode,
            name=f"{cfg_dict.wandb.name} ({output_dir.parent.name}/{output_dir.name})",
            tags=cfg_dict.wandb.get("tags", None),
            log_model="all",
            save_dir=output_dir,
            config=OmegaConf.to_container(cfg_dict),
            experiment=run
        )
        callbacks.append(LearningRateMonitor("step", True))

        # On rank != 0, wandb.run is None.
        if wandb.run is not None:
            print('saving code.............')
            # wandb.run.log_code("src")
            os.mkdir(run.dir+"/code")
            os.system(f"cp -r src {run.dir}/code/")
            os.system(f"cp -r config {run.dir}/code/")
            os.system(f"cp -r modules {run.dir}/code/")
    else:
        logger = LocalLogger()

    # print('+++++++++++++++++++logger:', logger)
    # Set up checkpointing.
    callbacks.append(
        ModelCheckpoint(
            output_dir / "checkpoints",
            every_n_train_steps=cfg.checkpointing.every_n_train_steps,
            save_top_k=cfg.checkpointing.save_top_k,
        )
    )

    # Prepare the checkpoint for loading.
    checkpoint_path = update_checkpoint_path(cfg.checkpointing.load, cfg.wandb)

    N = 10
    # This allows the current step to be shared with the data loader processes.
    step_tracker = StepTracker()

    trainer = Trainer(
        max_epochs=-1,
        accelerator="gpu",
        logger=logger,
        devices="auto",
        strategy="ddp_find_unused_parameters_true"
        if torch.cuda.device_count() > 1
        else "auto",
        callbacks=callbacks,
        val_check_interval=cfg.trainer.val_check_interval,
        enable_progress_bar=False,
        gradient_clip_val=cfg.trainer.gradient_clip_val,
        max_steps=cfg.trainer.max_steps,
        check_val_every_n_epoch=None,
        # precision=16 if cfg.model.encoder.backbone.name=='dino' else 32,
        # precision=32 if cfg.model.encoder.backbone.name=='efficientnet' else 16,
    )

    encoder, encoder_visualizer = get_encoder(cfg.model.encoder)
    cfg.test.output_path = output_dir

    model_wrapper = ModelWrapper(
        cfg.optimizer,
        cfg.test,
        cfg.train,
        encoder,
        encoder_visualizer,
        get_decoder(cfg.model.decoder, cfg.dataset),
        get_losses(cfg.loss),
        step_tracker,
        cfg_dict=cfg_dict,
        run_dir=run.dir,
        test_fvs=cfg.mode=='test_fvs',
        num_context_views=cfg.dataset.view_sampler.num_context_views,
    )
    if not cfg_dict.strict:
        current_model_dict = model_wrapper.state_dict()
        loaded_state_dict = torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict']
        new_state_dict={k:v if v.size()==current_model_dict[k].size() else current_model_dict[k] for k,v in zip(current_model_dict.keys(), loaded_state_dict.values())}
        model_wrapper.load_state_dict(new_state_dict, strict=False)
        checkpoint_path = None

    data_module = DataModule(cfg.dataset, cfg.data_loader, step_tracker, fvs=cfg.mode=='test_fvs')

    if cfg.mode == "train":
        trainer.fit(model_wrapper, datamodule=data_module, ckpt_path=checkpoint_path)
    else:
        trainer.test(
            model_wrapper,
            datamodule=data_module,
            ckpt_path=checkpoint_path,
        )


if __name__ == "__main__":
    train()
